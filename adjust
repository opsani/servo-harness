#!/usr/bin/env python3
import json
import math
import os
import requests
import sys
import time
import yaml
import subprocess

from adjust import Adjust, AdjustError

DESC = "Harness adjust driver for Opsani Optune"
VERSION = "0.0.1"
HAS_CANCEL = False

DEFAULTS = {
    'k8s': {    
        'mem': {
            'min': 0.125, # GiB
            'max': 16, # GiB
            'step': 0.125 # GiB
        },
        'cpu': {
            'min': 0.125, # cores
            'max': 4, # cores
            'step': 0.125 # cores
        }
    },
    'ecs': {    
        'mem': {
            'min': 0.25, # GiB
            'max': 4, # GiB
            'step': 0.25 # GiB
        },
        'cpu': {
            'min': 0.25, # vCPU
            'max': 4, # vCPU
            'step': 0.25 # vCPU
        }
    }
}

DEFAULT_ADJUST_TIMEOUT=3600

config_path = os.environ.get('OPTUNE_CONFIG', './config.yaml')
config_name = os.path.basename(__file__) if os.environ.get('OPTUNE_USE_DRIVER_NAME') else 'harness'

query_application_id = """
query {{
  applicationByName(name:"{}"){{
    id
  }}
}}
"""

query_canary_workflow_id = """
query {{
  workflowByName(applicationId:"{}",workflowName:"opsani-canary-deploy"){{
    id
    name
  }}
}}
"""

query_mainline_pipeline_id = """
query {{
  pipelineByName(applicationId:"{}",pipelineName:"Opsani Promotion"){{
    id
    name
  }}
}}
"""

query_webhook_url = """
query {{
  triggerByName(applicationId:"{}",triggerName: "opsani-canary-rollout") {{
    id
    condition {{
      ... on OnWebhook {{
        webhookDetails {{
          webhookURL
        }}
      }}
    }}
  }}
}}
"""

query_execution_status = """
query {{
  execution(executionId: "{}") {{
    createdAt
    startedAt
    endedAt
    notes
    status
  }}
}}
"""

canary_adjust_mutation = """
mutation {{
  startExecution(input: {{
    applicationId: "{applicationId}"
    entityId: "{entityId}"
    executionType: WORKFLOW,
    variableInputs: [
      {{
        name: "name"
        variableValue: {{
          type: NAME
          value: "canary"
        }}
      }},
      {{
        name: "cpu"
        variableValue: {{
          type: NAME
          value: "{cpu}"
        }}
      }},
      {{
        name: "mem"
        variableValue: {{
          type: NAME
          value: "{mem}"
        }}
      }},
      {{
        name: "InfraDefinition_ECS"
        variableValue: {{
          type: NAME
          value: "{InfraDefinition_ECS}"
        }}
      }},
      {{
        name: "Service"
        variableValue: {{
          type: NAME
          value: "{Service}"
        }}
      }}
    ]
    serviceInputs: [ {{
      name: "{Service}",
      artifactValueInput: {{
        valueType: BUILD_NUMBER
        buildNumber: {{
          buildNumber: "12"
          artifactSourceName: "{artifactSourceName}"
        }}
      }}
    }}
    ]
    notes: "Triggered by Opsani Engine",
  }}) {{
    warningMessage
    execution{{
      id
    }}
  }}
}}
"""

mainline_adjust_mutation = """
mutation {{
  startExecution(input: {{
    applicationId: "{applicationId}"
    entityId: "{entityId}"
    executionType: PIPELINE,
    variableInputs: [
      {{
        name: "cpu"
        variableValue: {{
          type: NAME
          value: "{cpu}"
        }}
      }},
      {{
        name: "mem"
        variableValue: {{
          type: NAME
          value: "{mem}"
        }}
      }}
    ]
    notes: "Triggered by Opsani Engine",
  }}) {{
    warningMessage
    execution{{
      id
    }}
  }}
}}
"""

class HarnessDriver(Adjust):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not (self.args.info or self.args.version):
            self.load_config()

    def load_config(self):
        try:
            with open(config_path) as in_file:
                c = yaml.safe_load(in_file)
        except yaml.YAMLError as e:
            raise Exception('Could not parse config file located at "{}". '
                            'Please check its contents. Error: {}'.format(config_path, str(e)))

        assert c and c.get(config_name, None), 'Harness Servo Configuration named {} was not provided in "{}". ' \
                                        'Please refer to README.md'.format(config_name, config_path)

        h = c[config_name]
        assert h.get('opsani_account'), 'No opsani_account was included in harness configuration located at {}'.format(config_path)
        assert h.get('opsani_app_name'), 'No opsani_app_name was included in harness configuration located at {}'.format(config_path)
        assert h.get('target_platform'), 'No target_platform was included in harness configuration located at {}'.format(config_path)
        assert h['target_platform'] in ['k8s', 'ecs'],\
          'Specified target_platform {} is not supported (in config located at{}). Supported values are "ecs" and "k8s"'.format(h['target_platform'], config_path)
        assert (h.get('adjust_interface', 'graphql') in ['webhook', 'graphql']) and (h.get('promote_interface', 'graphql') in ['webhook', 'graphql']),\
          'Invalid adjust_interface ({}) or promote_interface ({}) specified, only webhook and graphql are supported in config located at {}'\
          ''.format(h.get('adjust_interface', 'graphql'),h.get('promote_interface', 'graphql'), config_path)        
        assert h.get('account_id'), 'No account_id was included in harness configuration located at {}'.format(config_path)

        assert h.get('application_name'), 'No application_name was included in harness configuration located at {}'.format(config_path)
        assert h.get('graphql_url'), 'No graphql_url was included in harness configuration located at {}'.format(config_path)
        if h.get('adjust_interface', 'graphql') == 'graphql':
            assert h.get('infradefinition_ecs'), 'No infradefinition_ecs was included in harness configuration located at {}'.format(config_path)
            assert h.get('service'), 'No service was included in harness configuration located at {}'.format(config_path)
            assert h.get('artifact_source_name'), 'No artifact_source_name was included in harness configuration located at {}'.format(config_path)

        defaults = DEFAULTS[h['target_platform']]

        bad_keys = h.get('settings', {}).keys() - {'cpu', 'mem'}
        assert len(bad_keys) < 1, "harness settings config (located at {}) was malformed, contained unknown setting key(s) {}".format(config_path, ', '.join(bad_keys))

        for k,v in h.get('settings', {}).items():
            bad_keys = v.keys() - {'min', 'max', 'step', 'default'}
            assert len(bad_keys) < 1, "range setting {} config was malformed, contained unknown key(s) {}".format(k, ', '.join(bad_keys))
            assert isinstance(v.get('min', defaults[k]['min']), (int, float)), 'range setting {} config was malformed; min must be a number. Found {}.'.format(k, v.get('min'))
            assert isinstance(v.get('max', defaults[k]['max']), (int, float)), 'range setting {} config was malformed; max must be a number. Found {}.'.format(k, v.get('max'))
            assert isinstance(v.get('step', defaults[k]['step']), (int, float)), 'range setting {} config was malformed; step must be a number. Found {}.'.format(k, v.get('step'))
            # TODO: require default for settings?
            assert isinstance(v.get('default', 0), (int, float)), 'range setting {} config was malformed; default must be a number. Found {}.'.format(k, v.get('default'))
                
            assert v.get('min', defaults[k]['min']) <= v.get('max', defaults[k]['max']), \
                'range setting {} config was malformed; supplied/default min ({}) is higher than supplied/default max ({})'\
                    ''.format(k, v.get('min', defaults[k]['min']), v.get('max', defaults[k]['max']))

            if v.get('min', defaults[k]['min']) != v.get('max', defaults[k]['max']):
                assert v.get('step', defaults[k]['step']) != 0, 'range setting {} config was malformed; step cannot be zero when min != max.'.format(k)
                assert v.get('step', defaults[k]['step']) > 0, 'range setting {} config was malformed; step must be a positive number.'.format(k)

                c = (v.get('max', defaults[k]['max']) - v.get('min', defaults[k]['min'])) / float(v.get('step', defaults[k]['step']))
                assert math.isclose(c, round(c, 0), abs_tol = 1/1024), 'range setting {} config was malformed; step value must allow to get from {} to {} in equal steps of {}.'\
                    ''.format(k, v.get('min', defaults[k]['min']), v.get('max', defaults[k]['max']), v.get('step', defaults[k]['step']))

        self.config = h

        with open('/etc/harness-auth/token', 'r') as auth_file:
            harness_api_key = auth_file.read().strip()

        session = requests.Session()
        session.verify = self.config.get('ssl_verify', True)
        session.headers = {
            "X-API-KEY": harness_api_key,
            "content-type": "application/json",
        }
        session.params = { 'accountId': self.config['account_id'] }
        self.post_gql_query = lambda query_payload: session.post(url=self.config['graphql_url'], json={'query': query_payload })

        if h.get('adjust_interface', 'graphql') == 'webhook' or h.get('promote_interface', 'graphql') == 'webhook':
            self.trigger_session = requests.Session()
            self.trigger_session.headers = {"content-type": "application/json"}
            self.trigger_session.params = { 'accountId': self.config['account_id'] }
            self.trigger_session.verify = self.config.get('ssl_verify', True)
            self.status_session = requests.Session()
            self.status_session.verify = self.config.get('ssl_verify', True)
            self.status_session.headers = {
                "X-API-KEY": harness_api_key,
                "content-type": "application/json",
            }
            

    def query(self):
        defaults = DEFAULTS[self.config['target_platform']]
        settings = {
            'cpu': {
                'type': 'range',
                'unit': 'cores' if self.config['target_platform'] == 'k8s' else 'vCPU',
                'min': self.config.get('settings', {}).get('cpu', {}).get('min', defaults['cpu']['min']),
                'max': self.config.get('settings', {}).get('cpu', {}).get('max', defaults['cpu']['max']),
                'step': self.config.get('settings', {}).get('cpu', {}).get('step', defaults['cpu']['step'])
            },
            'mem': {
                'type': 'range',
                'unit': 'GiB',
                'min': self.config.get('settings', {}).get('mem', {}).get('min', defaults['mem']['min']), 
                'max': self.config.get('settings', {}).get('mem', {}).get('max', defaults['mem']['max']), 
                'step': self.config.get('settings', {}).get('mem', {}).get('step', defaults['mem']['step'])
            }
        }

        comp_key = 'canary' if config_name == 'harness' else config_name
        url = "https://api.opsani.com/accounts/{}/applications/{}/assets/opsani.com/{}".format(
            self.config['opsani_account'],
            self.config['opsani_app_name'],
            comp_key
        )

        with open('/etc/opsani-auth/token', 'r') as auth_file:
            opsani_token = auth_file.read().strip()
        headers = {
            "Content-type": "application/json",
            "Authorization": "Bearer {}".format(opsani_token),
        }
        response = requests.get(url, headers=headers)
        if not response.ok:
            raise AdjustError('Unable to query OCO config. Status code {}. Data {} comp_key {}'.format(response.status_code, response.text, comp_key))

        opsani_config = response.json()
        opsani_config_settings = opsani_config.get('data')

        if opsani_config_settings and opsani_config_settings.get('cpu'):
            if self.config['target_platform'] == 'k8s':
                settings['cpu']['value'] = cpuunits(opsani_config_settings['cpu'])
            elif self.config['target_platform'] == 'ecs':
                settings['cpu']['value'] = float(opsani_config_settings['cpu']) / 1024 # convert aws cpu units to vCPU
        else:
            # TODO: require default in config validation or implement default values for setting defaults
            settings['cpu']['value'] = float(self.config.get('cpu', {}).get('default'))
        
        if opsani_config_settings and opsani_config_settings.get('mem'):
            if self.config['target_platform'] == 'k8s':
                settings['mem']['value'] = memunits(opsani_config_settings['mem'])
            elif self.config['target_platform'] == 'ecs':
                settings['mem']['value'] = float(opsani_config_settings['mem']) / 1024 # convert MiB to GiB
        else:
            settings['mem']['value'] = float(self.config.get('mem', {}).get('default'))

        return { 'application': { 'components': { comp_key: { 'settings': settings } } } }

    def adjust(self, data):
        if self.config.get('adjust_on') and data["control"]["userdata"]["deploy_to"] != self.config['adjust_on']:
            return {"status": "ok", "reason": "Skipped due to 'adjust_on' condition"}

        canary_settings = data['application']['components'].get('canary', {}).get('settings')
        mainline_settings = data['application']['components'].get('mainline', {}).get('settings')
        if not canary_settings and not mainline_settings:
            return {"status": "ok", "reason": "Skipped due to no data"}
        
        response = self.post_gql_query(query_payload=query_application_id.format(self.config["application_name"]))
        if not response.ok or response.json().get('errors'):
            raise AdjustError('Unable to get harness application id. Status code {}. Data {}'.format(response.status_code, response.text))
        app_id = response.json().get('data', {}).get('applicationByName', {}).get('id')
        if app_id is None:
            raise AdjustError('Unable to parse harness application id from response json. Data: {}'.format(response.json()))

        if canary_settings:
            weights = [0,1,10,25,50,75,100]
            cpu_val, mem_val = self._parse_input_values(canary_settings)
            input_data = json.dumps(data)
            for weight in weights:
              if self.config.get('adjust_interface', 'graphql') == 'graphql':
                  self._adjust_graphql(cpu_val, mem_val, app_id, 'canary', weight)
                  subprocess.run(['./measure', app_id, input_data], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                  time.sleep(300)
              else:
                  self._adjust_webhook(cpu_val, mem_val, app_id, 'canary',weight)
                  subprocess.run(['./measure', app_id, input_data], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                  time.sleep(300)
                
        if mainline_settings:
            cpu_val, mem_val = self._parse_input_values(mainline_settings)
            weight = 100
            if self.config.get('promote_interface', 'graphql') == 'graphql':
                self._adjust_graphql(cpu_val, mem_val, app_id, 'mainline', weight)
            else:
                self._adjust_webhook(cpu_val, mem_val, app_id, 'mainline', weight)

    def _parse_input_values(self, settings):
        'Retrieve cpu and mem values from input json and convert to target platform units based on loaded config'
        if self.config['target_platform'] == 'k8s':
            cpu_val = settings['cpu']['value']
            mem_val = '{}Gi'.format(settings['mem']['value'])
        elif self.config['target_platform'] == 'ecs':
            cpu_val = str(int(settings['cpu']['value'] * 1024)) # convert vCPU to cpu units
            mem_val = str(int(settings['mem']['value'] * 1024)) # convert GiB to MiB
        return cpu_val, mem_val
        
    def _adjust_webhook(self, cpu, mem, app_id, target, weight):
        assert target in ['canary', 'mainline'], 'Invalid target {} given to  _adjust_webhook'.format(target)
        self.progress_message = 'Preparing to trigger {} worlfow via webhook'.format(target) # Don't print unless this part takes longer than 30s

        response = self.post_gql_query(query_webhook_url.format(app_id))
        if not response.ok or response.json().get('errors'):
            raise AdjustError('Unable to get {} harness webhook URL. Status code {}. Data {}'.format(target, response.status_code, response.text))

        trigger_url = response.json().get('data', {}).get('triggerByName', {}).get('condition', {}).get('webhookDetails', {}).get('webhookURL')
        trigger_url = trigger_url.replace("/gateway/api/","/api/")
        if trigger_url is None:
            raise AdjustError('Unable to parse {} harness webhook URL from response json. Data: {}'.format(target, response.json()))

        trigger_json_data = {
            "application": app_id,
            "parameters": { "cpu": cpu, "mem": mem, "canaryWeight": weight },
        }
        if target == 'mainline':
            trigger_json_data['promote'] = 'True'

        self.progress_message = 'Triggering {} webhook'.format(target)
        self.print_progress()

        response = self.trigger_session.post(url=trigger_url, json=trigger_json_data)
        if not response.ok:
            raise AdjustError('Unable to trigger {} harness workflow webhook. Status code {}. Data {}. Trigger URL {}. Trigger JSON {}.'.format(target, response.status_code, response.text, trigger_url, trigger_json_data))

        trigger_result = response.json()
        if not trigger_result.get('apiUrl'):
            raise AdjustError('Response for {} workflow trigger response contained no status url. Response: {}'.format(target, response.text))

        status_url = trigger_result["apiUrl"].replace("https://app.harness.io/api/","https://app.harness.io/gateway/api/")
        self.progress_message = 'Waiting for {} workflow status SUCCESS'.format(target)
        start_time = time.time()
        while True:
            status_response = self.status_session.get(url=status_url)
            if not status_response.ok:
                raise AdjustError('Unable to get triggered {} workflow status. Status code {}. Data {}'.format(target, status_response.status_code, status_response.text))
            
            status_json = status_response.json()
            if status_json['status'] == 'SUCCESS':
                break
            if status_json['status'] == 'FAILED':
                raise AdjustError('Triggered {} workflow finished with FAILED status: {}'.format(target, status_json))
            # else: status == 'RUNNING'

            if time.time() - start_time > self.config.get('adjust_timeout', DEFAULT_ADJUST_TIMEOUT):
                raise AdjustError('Timed out waiting for completion of {} adjustment'.format(target))

            time.sleep(30)

    def _adjust_graphql(self, cpu, mem, app_id, target, weight):
        assert target in ['canary', 'mainline'], 'Invalid target {} given to  _adjust_graphql'.format(target)
        self.progress_message = 'Preparing to trigger {} worlfow via graphql'.format(target) # Don't print unless this part takes longer than 30s

        if target == 'canary':
            response = self.post_gql_query(query_payload=query_canary_workflow_id.format(app_id))
        else: # target == 'mainline'
            response = self.post_gql_query(query_payload=query_mainline_pipeline_id.format(app_id))

        if not response.ok or response.json().get('errors'):
            raise AdjustError('Unable to get {} harness {} id. Status code {}. Data {}'\
                .format(target, 'workflow' if target == 'canary' else 'pipeline', response.status_code, response.text))

        if target == 'canary':
            entity_id = response.json().get('data', {}).get('workflowByName', {}).get('id')
        else: # target == 'mainline'
            entity_id = ((response.json().get('data') or {}).get('pipelineByName') or {}).get('id')

        if entity_id is None:
            raise AdjustError('Unable to parse {} harness {} id from response json. Data: {}'\
                .format(target, 'workflow' if target == 'canary' else 'pipeline', response.json()))

        kwargs = {
            'applicationId': app_id,
            'entityId': entity_id,
            'cpu': cpu,
            'mem': mem
        }
        if target == 'canary':
            kwargs['InfraDefinition_ECS'] = self.config["infradefinition_ecs"]
            kwargs['Service'] = self.config["service"]
            kwargs['artifactSourceName'] = self.config["artifact_source_name"]
        
        self.progress_message = 'Triggering {} worlfow via graphql'.format(target)
        self.print_progress()

        if target == 'canary':
            response = self.post_gql_query(query_payload=canary_adjust_mutation.format(**kwargs))
        else:
            response = self.post_gql_query(query_payload=mainline_adjust_mutation.format(**kwargs))
        
        if not response.ok or response.json().get('errors'):
            raise AdjustError('Unable to {} canary harness workflow. Status code {}. Data {}'.format(target, response.status_code, response.text))

        se = response.json().get('data', {}).get('startExecution', {})
        execution_id = se.get('execution', {}).get('id')
        if execution_id is None:
            raise AdjustError('Failed to parse execution id from workflow trigger response. Response: {}'.format(response.json()))

        if se.get('warningMessage'):
            print('Trigger response contained warning: {}'.format(se['warningMessage']), file=sys.stderr)

        self.progress_message = 'Waiting for workflow status SUCCESS or timeout'
        self.print_progress()
        start_time = time.time()
        while True:
            status_response = self.post_gql_query(query_payload=query_execution_status.format(execution_id))
            if not status_response.ok or status_response.json().get('errors'):
                raise AdjustError('Unable to get triggered workflow execution status. Status code {}. Data {}'.format(status_response.status_code, status_response.text))
            
            execution_json = status_response.json().get('data', {}).get('execution')
            if execution_json is None:
                raise AdjustError('Unable to parse triggered workflow execution status. Data {}'.format(status_response.json()))

            if execution_json['status'] == 'SUCCESS':
                break
            if execution_json['status'] == 'FAILED':
                raise AdjustError('Triggered workflow finished with FAILED status: {}'.format(execution_json))
            # else: status == 'RUNNING'

            if time.time() - start_time > self.config.get('adjust_timeout', DEFAULT_ADJUST_TIMEOUT):
                if target == 'mainline':
                    break # Timeouts only error for canary. We don't need to wait for mainline to finish
                raise AdjustError('Timed out waiting for completion of adjustment')

            time.sleep(30)

def cpuunits(s) -> float:
    '''convert a string for CPU resource (with optional unit suffix) into a number'''
    if s[-1] == "m": # there are no units other than 'm' (millicpu)
        return float(s[:-1])/1000.0
    return float(s)

# valid mem units: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki
# nb: 'm' suffix found after setting 0.7Gi
mumap = {"E":1000**6,  "P":1000**5,  "T":1000**4,  "G":1000**3,  "M":1000**2,  "K":1000, "m":1000**-1,
         "Ei":1024**6, "Pi":1024**5, "Ti":1024**4, "Gi":1024**3, "Mi":1024**2, "Ki":1024}
Gi = 1024 * 1024 * 1024

def memunits(s) -> float:
    '''convert a string for memory resource (with optional unit suffix) into a number'''
    for u, m in mumap.items():
        if s.endswith(u):
            return (float(s[:-len(u)]) * m) / Gi
    return float(s) / Gi

if __name__ == '__main__':
    driver = HarnessDriver(cli_desc=DESC, supports_cancel=HAS_CANCEL, version=VERSION, progress_interval=30)
    driver.run()
